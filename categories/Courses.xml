<?xml version="1.0" encoding="UTF-8"?> <feed xmlns="http://www.w3.org/2005/Atom">
  <title>Thomas Kappler&#39;s site. Mostly programming and books. Category &#34;Courses.&#34;</title>
  <link href="http://www.thomaskappler.net/categories/Courses/" rel="alternate"></link>
  <id>http://www.thomaskappler.net/categories/Courses/</id>
  <updated>2013-05-25T19:50:35+02:00</updated>
  <author>
   <name>Thomas Kappler</name>
   <uri>http://www.thomaskappler.net/</uri>
  </author>
  <entry>
   <title>Intro to Artificial Intelligence Online Course</title>
   <link href="http://www.thomaskappler.net/2011-12-22_intro_to_ai_course.html" rel="alternate"></link>
   <updated>2011-12-22T00:00:00Z</updated>
   <id>tag:www.thomaskappler.net,2011-12-22:/2011-12-22_intro_to_ai_course.html</id>
   <summary type="html">The online class was a bold experiment, bringing Stanford teaching by Sebastian Thun and Peter Norvig to over 100,000 students. I loved the experience and got a lot out of it. Here are some thoughts and selected notes.</summary>
   <content type="html">&lt;p&gt;I had always been interested in AI but ended up choosing different&#xA;majors in college. Naturally, this&#xA;&lt;a href=&#34;https://www.ai-class.com/&#34;&gt;online AI class&lt;/a&gt; couldn&amp;rsquo;t make up for that&#xA;by itself given the time constraints, but I feel it did a great job on&#xA;bringing the students up to speed in the important basics of the&#xA;field. The range of topics was wide enough to serve as a solid&#xA;foundation for further reading:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;How to think of environments&lt;/li&gt;&#xA;&lt;li&gt;Searching through state spaces, A* search&lt;/li&gt;&#xA;&lt;li&gt;Probability and Bayes Rule&lt;/li&gt;&#xA;&lt;li&gt;Bayes Networks, D-Separation and probabilistic inference&lt;/li&gt;&#xA;&lt;li&gt;Sampling methods&lt;/li&gt;&#xA;&lt;li&gt;Machine Learning basics and k-nearest neighbors&lt;/li&gt;&#xA;&lt;li&gt;Planning under uncertainty, Markov Decision Processes&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The online format worked great. I enjoyed the little quizzes during&#xA;the course of each lecture because they made me pay more attention and&#xA;helped retention.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Due to work and other constraints I couldn&amp;rsquo;t give the class as much&#xA;time as I wanted, but I still finished with a score of 88%. Nothing&#xA;special given that I have a CS degree, but not too bad either. In any&#xA;case I&amp;rsquo;m proud that I took the time to sit down every week and do&#xA;this, and of having been part of this huge experiment that will&#xA;probably shape the future of learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below are some of my class notes. They are not complete and you&amp;rsquo;d be&#xA;better off taking the class yourself&amp;mdash;it&amp;rsquo;s&#xA;&lt;a href=&#34;http://www.udacity.com/overview/Course/cs271&#34;&gt;available on udacity&lt;/a&gt;&#xA;now.&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;h2&gt;1 Welcome to AI&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;AI in a nutshell: an agent observes its environments through sensors,&#xA;and can influence it through actuators. The &lt;em&gt;control plan&lt;/em&gt; mapping&#xA;sensor input to activator output is the focus of AI. The repeated&#xA;cycle of input and output is the &lt;em&gt;Perception Action Cycle&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Classifying environments:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;An environment is &lt;em&gt;fully observable&lt;/em&gt; if an agent can, at all&#xA;times, make all observations to make the optimal choice, i.e., see&#xA;the entire state of the environment (chess). If this is not&#xA;possible and the agent has to memorize previous observations&#xA;(poker), it is &lt;em&gt;partially observable&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;It can be deterministic (chess) or stochastic (dice games).&lt;/li&gt;&#xA;&lt;li&gt;The agent&amp;rsquo;s actuators can be discrete or continuous (e.g. throwing&#xA;darts: infinite possibilities of angles and acceleration).&lt;/li&gt;&#xA;&lt;li&gt;It can be benign or adversarial.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;2 Problem Solving&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Definition of a problem:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;initial state&lt;/li&gt;&#xA;&lt;li&gt;actions(s) = {a1, a2, &amp;hellip;}&lt;/li&gt;&#xA;&lt;li&gt;result(s, a) = s&amp;rsquo;&lt;/li&gt;&#xA;&lt;li&gt;goaltest(s) = T|F&lt;/li&gt;&#xA;&lt;li&gt;pathcost(s-a-&amp;gt;s-a-&amp;gt;s-&amp;hellip;) -&amp;gt; n&lt;/li&gt;&#xA;&lt;li&gt;stepcost(s, a, s&amp;rsquo;) -&amp;gt; n&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;State space&lt;/em&gt; = explored + frontier + unexplored&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Tree search&lt;/em&gt;: until the goal is reached, take a choice from the&#xA;frontier and apply all actions to it. This is a whole family of&#xA;algorithms, since the removal of the choice is critical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In breadth-first tree search, the three sections of the state space&#xA;correspond to three vertical regions of the tree.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tree search backtracks, since for each tree node, the one we came from&#xA;is just another frontier node.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can turn the tree search into a graph search by remembering which&#xA;nodes we&amp;rsquo;ve seen already.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In &lt;em&gt;uniform cost search&lt;/em&gt; we pick the cheapest choice first.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For finding the goal in the shortest route as in hops, breadth-first&#xA;is optimal while depth-first is not. For finding the goal via the&#xA;cheapest route, uniform cost is optimal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Uniform cost search is related to topological maps: we fan out from a&#xA;starting point, in circles of increasing distance from the starting&#xA;point.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;A* search&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;f = g + h&lt;/code&gt; where g(p) is the path cost and h(p) = h(final state of p)&#xA;is the estimated distance to the goal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A* search does not end when the goal is reached&amp;mdash;there might be&#xA;other, shorter paths. The goal test happens after selecting the next&#xA;path to be taken off the frontier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A* finds the lowest cost path if, for all s, h(s) &amp;lt;= true cost. In&#xA;other words, h is &lt;em&gt;optimistic&lt;/em&gt; or &lt;em&gt;admissible&lt;/em&gt;, it never overestimates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Problem solving works when:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the domain is fully observable,&lt;/li&gt;&#xA;&lt;li&gt;the domain is known (available actions),&lt;/li&gt;&#xA;&lt;li&gt;the domain is discrete (finite number of actions),&lt;/li&gt;&#xA;&lt;li&gt;the domain is deterministic, and&lt;/li&gt;&#xA;&lt;li&gt;the domain is static.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;For the implementation of paths and state spaces the core data&#xA;structure is the &lt;em&gt;node&lt;/em&gt;, with four fields: state, action (it took to&#xA;get there), cost (total), parent (preceding state). A linked list of&#xA;nodes represents a path. The two main data structures dealing with&#xA;nodes are the frontier and the explored list. The frontier is a&#xA;priority queue with an additional set for membership test. The&#xA;explored list can be a simple set.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;3 Probability in AI&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Bayes Networks&lt;/em&gt;: nodes are random variables. Edges signify&#xA;probabilistic influence. It&amp;rsquo;s a compact representation of a&#xA;probability distribution over a very large joint distribution of all&#xA;involved variables. The state space has the size 2^n even if the&#xA;variables were all binary.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If X and Y are independent, &lt;code&gt;P(X,Y) = P(X)*P(Y)&lt;/code&gt;. The P(X) and P(Y)&#xA;are called &lt;em&gt;marginals&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, some simple probability review:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Total probability: &lt;code&gt;P(Y) = P(Y|X=i)*P(X=i)&lt;/code&gt; summed over all i.&lt;/li&gt;&#xA;&lt;li&gt;Negation: &lt;code&gt;P(-X|Y) = 1 - P(X|Y)&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Joint probabilities: &lt;code&gt;P(X,Y) = P(Y=y|X=x) * P(X=x)&lt;/code&gt; and vice&#xA;versa.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Bayes Rule&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;P(A|B) = P(B|A)\*P(A) / P(B)&lt;/code&gt;. In this formula, P(B|A) is the&#xA;&lt;em&gt;likelihood&lt;/em&gt;, P(A) the &lt;em&gt;prior&lt;/em&gt;, P(B) the &lt;em&gt;marginal likelihood&lt;/em&gt;, and&#xA;P(A|B) the &lt;em&gt;posterior&lt;/em&gt;. The marginal likelihood can be replaced by the&#xA;total probability.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Drawing Bayes Rule with two nodes A and B is a Bayesian network with&#xA;two nodes A-&amp;gt;B!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How many parameters do we need to specify a Bayes Network of two&#xA;variables? Three: P(A) (from which we derive P(-A)), P(B|A), and&#xA;P(B|-A).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Observe that using Bayes Rule, the normalizer is the same for both&#xA;P(A|B) and P(-A|B). The two cases sum to 1. So we can compute Bayes&#xA;Rule in a different way, effectively ignoring the normalizer:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;P&#39;(A|B)  = P(B|A)  P(A)&#xA;P&#39;(-A|B) = P(B|-A) P(-A)&#xA;&#xA;P(A|B)  = &amp;lt;eta&amp;gt;P&#39;(A|B)&#xA;P(-A|B) = &amp;lt;eta&amp;gt;P&#39;(-A|B)&#xA;&#xA;&amp;lt;eta&amp;gt; = 1 / (P&#39;(A|B) + P&#39;(-A|B))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We defer the calculation of the normalizer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Exploiting &lt;em&gt;conditional independence&lt;/em&gt;: if T1 and T2 with values x and&#xA;y depend on A, what is P(t1|t2), given P(A), P(x|A) and P(y|A)? Using&#xA;total probability, it is &lt;code&gt;P(t2|t1,A) * P(A|t1) + P(t2|t1,-A) *&#xA;P(-A|t1)&lt;/code&gt;. But if we know A and T1 and T2 are independent, knowledge&#xA;of the first test gives me no more information about the second test.&#xA;So we can simplify this to &lt;code&gt;P(t2|A)P(A|t1) + P(t2|-A)P(-A|t1)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Absolute independence does not imply conditional independence, and&#xA;vice versa.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Confounding causes&lt;/em&gt;: two (unobservable) nodes influence one output&#xA;variable. There&amp;rsquo;s no connection between the two influencing&#xA;variables&amp;mdash;if P(A) = x, then P(A|B=b) = x.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Explaining away&lt;/em&gt;: independence does not imply conditional&#xA; independence. If we have&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;S   R&#xA; \ /&#xA;  H&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;then S and R are independent. But if we observe H, they are not&#xA;independent anymore&amp;mdash;its value gives us a clue which one is more&#xA;likely.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Bayes Networks&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The joint probability of a BN is the product of the probabilities of&#xA;its nodes, conditioned on their incoming arcs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How many parameters define a BN? If the above product is &lt;code&gt;P(A) P(B)&#xA;P(C|A,B) P(D|C) P(E|C)&lt;/code&gt;, it&amp;rsquo;s ten. P(A) and P(B) are one each. P(D|C)&#xA;and P(E|C) are two each, for each possible value of C. P(C|A,B) is 4:&#xA;all combinations of A and B.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;D-Separation&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;This is about &lt;em&gt;conditional independence&lt;/em&gt; in BNs, also called&#xA;&lt;em&gt;reachability&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To determine independence of variables in a graph (like a BN), think&#xA;about &amp;ldquo;would knowing X help me to know more about Y?&amp;rdquo; More formally,&#xA;variables are dependent if they are connected by a path of &lt;em&gt;unknown&lt;/em&gt;&#xA;variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;A -&amp;gt; UNKNOWN -&amp;gt; B&lt;/code&gt; is an &lt;em&gt;active triplet&lt;/em&gt;, &lt;code&gt;A -&amp;gt; KNOWN -&amp;gt; B&lt;/code&gt; is&#xA;&lt;em&gt;inactive&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This also goes for upstream relations&amp;mdash;this is the &amp;ldquo;explaining away&amp;rdquo;&#xA;case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&#34;http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html&#34;&gt;Bayes Ball&lt;/a&gt; for&#xA;an easy test of conditional independence in BNs.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;4 Probabilistic Inference&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Given a BN, we can ask &amp;ldquo;for these inputs, what are the outputs?&amp;rdquo; We&#xA;call the input &lt;em&gt;evidence&lt;/em&gt;, the output &lt;em&gt;query&lt;/em&gt;, and the interior nodes&#xA;&lt;em&gt;hidden variables&lt;/em&gt;. The answer is a complete probability distribution&#xA;over the query variables, called the &lt;em&gt;posterior distribution&lt;/em&gt;: P(Q1,&#xA;Q2, &amp;hellip;|E1=e1, &amp;hellip;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Evidence nodes whose value we don&amp;rsquo;t know, and query nodes we don&amp;rsquo;t&#xA;care about are hidden!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The obvious question is which evidence has the most likely result,&#xA;i.e., the argmax over the above P(&amp;hellip;).&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Enumeration&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Conditional probability&lt;/em&gt;: &lt;code&gt;P(Q|E) = P(Q,E) / P(E)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Enumerating a BN, here for the burglary/earthquake -&amp;gt; alarm -&amp;gt;&#xA;John/Mary calls network. We sum over the probability product for all&#xA;combinations of the hidden variables (&amp;ldquo;Se&amp;rdquo; = sum over e).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Se(Sa( P(+b) P(e) P(A|+b,e)P(+j|a)P(+m,a) ))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Moving invariants out of inner loops:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;P(+b) S( P(e) S(P(A|+b,e)P(+j|a)P(+m,a)) )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That&amp;rsquo;s still not good enough. The next step is to &lt;em&gt;maximize&#xA;independence&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A &amp;ldquo;linear&amp;rdquo; BN where each node points to one other node will take O(n),&#xA;while one where each node is connected to all other nodes will take&#xA;(2^n).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The next optimization is &lt;em&gt;variable elimination&lt;/em&gt; (4.8). First, joining&#xA;factors. Let&amp;rsquo;s say we have P&amp;reg; and P(T|R). We combine them into&#xA;P(R,T) by multiplying their tables together, through all combinations&#xA;of r and t. Then, elimination (also called marginalization): turn&#xA;P(R,T) into P(T) by summing up the P(R,T) table over r.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Approximate Inference Sampling&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Flip coin repeatedly, count outcomes to estimate the joint probability&#xA;distribution. In a BN&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the count of each sampled variable is chosen according to the probability&#xA;tables, the sampling approaches the true probability distribution with&#xA;infinite samples, then the sampling method is &lt;em&gt;consistent&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Rejection sampling&lt;/em&gt;: to estimate the distribution for a conditional&#xA;variable, we go through the samples and reject all those that don&amp;rsquo;t&#xA;match the condition (evidence). The problem is that if the evidence is&#xA;unlikely we need to reject most samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Likelihood weighting&lt;/em&gt; fixes this problem. We fix the evidence&#xA;variables and sample the rest, so we can keep all samples. However,&#xA;the distribution is inconsistent. To make it consistent, we attach a&#xA;probabilistic weight to each sample. For instance, if rain has a&#xA;probability of 0.4 and we&amp;rsquo;re sampling P(grass wet|rain), we are forced&#xA;to choose the 0.4 probability entry for rain in its table, so we&#xA;weight the sample with 0.4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Gibb&amp;rsquo;s Sampling&lt;/em&gt; takes all evidence into account. It&amp;rsquo;s based on&#xA;Markov Chain Monte Carlo (MCMC). The idea is to sample one variable at&#xA;a time, conditioned on all the others. We initialize all variables for&#xA;the first sample. In each following iteration, we choose one variable&#xA;at random and resample it. So we randomly walk the space of variable&#xA;assignments. Even though each iteration depends on the previous one,&#xA;the method is consistent.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Approximate Inference Sampling&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;We can Flip a coin repeatedly and count the outcomes to estimate the&#xA;joint probability distribution. If the count of each sampled variable&#xA;is chosen according to the probability tables, the sampling approaches&#xA;the true probability distribution with infinite samples, then the&#xA;sampling method is &lt;em&gt;consistent&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Rejection sampling&lt;/em&gt;: to estimate the distribution for a conditional&#xA;variable, we go through the samples and reject all those that don&amp;rsquo;t&#xA;match the condition (evidence). The problem is that if the evidence is&#xA;unlikely we need to reject most samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Likelihood weighting&lt;/em&gt; fixes this problem. We fix the evidence&#xA;variables and sample the rest, so we can keep all samples. However,&#xA;the distribution is inconsistent. To make it consistent, we attach a&#xA;probabilistic weight to each sample. For instance, if rain has a&#xA;probability of 0.4 and we&amp;rsquo;re sampling P(grass wet|rain), we are forced&#xA;to choose the 0.4 probability entry for rain in its table, so we&#xA;weight the sample with 0.4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Gibb&amp;rsquo;s Sampling&lt;/em&gt; takes all evidence into account. It&amp;rsquo;s based on&#xA;Markov Chain Monte Carlo (MCMC). The idea is to sample one variable at&#xA;a time, conditioned on all the others. We initialize all variables for&#xA;the first sample. In each following iteration, we choose one variable&#xA;at random and resample it. So we randomly walk the space of variable&#xA;assignments. Even though each iteration depends on the previous one,&#xA;the method is consistent.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;5 Machine Learning&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Machine learning&lt;/em&gt; is learning models from data. (Later note: it&amp;rsquo;s&#xA;interesting to compare this definition with Andrew Ng&amp;rsquo;s from the&#xA;Stanford Machine Learning class on coursera: giving computers the&#xA;ability to learn without being explicitly programmed.)&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Supervised Learning&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Given a &lt;em&gt;feature vector&lt;/em&gt; &lt;code&gt;x1, ..., xn -&amp;gt; y&lt;/code&gt;, y is the &lt;em&gt;target label&lt;/em&gt; or&#xA;&lt;em&gt;predictor&lt;/em&gt;. A set of feature vectors with their target labels is&#xA;called the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Occam&amp;rsquo;s Razor&lt;/em&gt;: prefer the simpler hypothesis. There is a conflict&#xA;between generalization error and training data error. Minimizing the&#xA;latter leads to overfitting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Maximum Likelihood&lt;/em&gt;: given a number of discrete data points y_i, find&#xA;the P(x) that maximizes the likelihood of x in the data. With two&#xA;discrete outcomes H and S: &lt;code&gt;p(S) = q&lt;/code&gt;, &lt;code&gt;p(y_i) = q&lt;/code&gt; if y_i=S, 1-q&#xA;otherwise. Then &lt;code&gt;p(data) = multover(p(y_i) = q^(count(y_i=H) *&#xA;(1-q)^(count(Y_i=S))&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can do the counts, now we have to determine q. Maximizing p(data)&#xA;is equivalent to maximizing log(p(data)).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can construct a BN modeling the problem. If we have a dictionary&#xA;with 12 words, the BN will have 23 parameters: 1 for P(spam), and 12&#xA;each for the spam and ham distributions. The parameters are estimated&#xA;by supervised learning using an ML estimator with training data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To determine whether a message is spam, given the prior P(S), we&#xA;multiply the prior with the probabilities of each word of occuring in&#xA;a spam message, and divide by this expression plus the equivalent one&#xA;for ham (total probability). Normal Bayes Theorem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When determing the p(w) that a word is spam by counting its occurences&#xA;in the test data spam, we risk overfitting. When a word does not occur&#xA;in the test data, the P(S) of a message containing that word will&#xA;always be 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Laplace smoothing&lt;/em&gt; addresses this. Instead of p(x) = count(x)/N, we&#xA;add a smoothing factor: &lt;code&gt;p(x) = count(x)+k / count(x) + N&lt;/code&gt;, where N is&#xA;the number of total different values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cross-validation: find the Laplacian k by cross-validating repeatedly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Supervised learning can be classification (what we&amp;rsquo;ve done until now),&#xA;and regression: make continuous predictions, such as for tomorrow&amp;rsquo;s&#xA;temperature.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suggestively: what&amp;rsquo;s the best curve through the data points? The one&#xA;that hits each point would be good, but it&amp;rsquo;s overfit of course.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Linear regression formally: Given x11, &amp;hellip;, x1n -&amp;gt; y1 to xm1, &amp;hellip;, xmn&#xA;-&amp;gt; ym, we want f(x) = y. In two dimensions: f(x) = w1*x + w0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We minimize the loss function modeling the residual error.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LOSS = sum_j(yj - w1*xj - w0)^2.&lt;/li&gt;&#xA;&lt;li&gt;w1* = argmin_w(L)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Because we minimize quadratic error, outliers penalize&#xA;disproportionately.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Logistic regression: if f(x) is a linear regression, z = 1 / 1 +&#xA;e^(-f(x)).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Gradient descent: iterative method. The i+1 guess is the ith guess&#xA;minus its gradient times a small alpha (0.01). Gradient descent gets&#xA;trapped in local minima.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which linear separator (if there is one) is preferrable? The one that&#xA;maximizes the margin, i.e., the distance to the closer data&#xA;points. This ensures that future real data will probably be on the&#xA;right side. SVMs and Boosting are popular to find this separator.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SVMs work with kernels to enable linear separation. For instance, if&#xA;points are circular around the origin, and class + is closer than&#xA;class -, there is no linear separator. But if we map the points to&#xA;their distance from origin, there is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, all methods of supervised learning were parametric. The number&#xA;of parameters was independent of the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In non-parametric methods, the number of params can grow over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Conformate&lt;/em&gt; plans are plans that are guaranteed to reach a goal&#xA;without sensing the world.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;9 Planning under uncertainty&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We&amp;rsquo;ve covered planning, uncertainty, and learning before, but not in&#xA;combination. Where planning and uncertainty intersect, we will look at&#xA;&lt;em&gt;(Partially observable) Markov Decision Processes&lt;/em&gt; (MDPs). In&#xA;combination with learning, we&amp;rsquo;ll look at &lt;em&gt;Reinforcement Learning&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;MDPs&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;We have states, actions, and a state transition matrix where&#xA;&lt;code&gt;T(s,a,s&#39;) = P(s&#39;|a,s)&lt;/code&gt;. A reward function R(s) defines the goal to&#xA;achieve. For now, the rewards are simply attached to states. The&#xA;problem is then to attach an action to each state so that R(s) is&#xA;maximized.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A &lt;em&gt;policy&lt;/em&gt; assigns actions to any state (except absorbing states).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a stochastic environment, what&amp;rsquo;s the problem with the tree approach&#xA;of conventional planning? Large branching factor, deep trees, many&#xA;states visited more than once.&lt;/p&gt;&#xA;</content>
   <category term="AI&amp;ML"></category>
   <category term="Courses"></category>
  </entry>
 </feed>