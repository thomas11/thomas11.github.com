<?xml version="1.0" encoding="UTF-8"?> <feed xmlns="http://www.w3.org/2005/Atom">
  <title>Thomas Kappler&#39;s site. Mostly programming and books. Category &#34;AI&amp;ML.&#34;</title>
  <link href="http://www.thomaskappler.net/categories/AI&amp;ML/" rel="alternate"></link>
  <id>http://www.thomaskappler.net/categories/AI&amp;ML/</id>
  <updated>2015-09-17T21:15:50+02:00</updated>
  <author>
   <name>Thomas Kappler</name>
   <uri>http://www.thomaskappler.net/</uri>
  </author>
  <entry>
   <title>Coursera Machine Learning with Andrew Ng</title>
   <link href="http://www.thomaskappler.net/2013-07-23_coursera_ml_class.html" rel="alternate"></link>
   <updated>2013-07-23T00:00:00Z</updated>
   <id>tag:www.thomaskappler.net,2013-07-23:/2013-07-23_coursera_ml_class.html</id>
   <summary type="html">I took the recent online machine learning class with Coursera. It was a good experience and I learned a lot. Here are some notes and observations.</summary>
   <content type="html">&lt;p&gt;Machine Learning is on my radar of things to learn since a long time.&#xA;After all, I&amp;rsquo;m already kind of a data guy in software development, but&#xA;I&amp;rsquo;m not very strong on the analysis and classification side.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Professor&#xA;&lt;a href=&#34;http://ai.stanford.edu/~ang/&#34;&gt;Andrew Ng of the Stanford AI lab&lt;/a&gt;&#xA;offers a&#xA;&lt;a href=&#34;https://class.coursera.org/ml-003/class/index&#34;&gt;Coursera class in Machine Learning&lt;/a&gt;.&#xA;I took the recent class from April to June 2013.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The class was a great experience. I found the lectures to be clear and&#xA;the content made perfect sense as an intro to machine learning. The&#xA;pace was often a bit slow for my taste, since I already knew the&#xA;required math, but this was easily fixed by watching the lectures at a&#xA;higher speed. The Coursera online video player offers this convenient&#xA;feature including resampling so the voice doesn&amp;rsquo;t sound funny.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This was the syllabus:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Linear Regression with One Variable&lt;/li&gt;&#xA;&lt;li&gt;Linear Regression with Multiple Variables&lt;/li&gt;&#xA;&lt;li&gt;Logistic Regression&lt;/li&gt;&#xA;&lt;li&gt;Regularization&lt;/li&gt;&#xA;&lt;li&gt;Neural Networks: Representation&lt;/li&gt;&#xA;&lt;li&gt;Neural Networks: Learning&lt;/li&gt;&#xA;&lt;li&gt;Advice for Applying Machine Learning&lt;/li&gt;&#xA;&lt;li&gt;Machine Learning System Design&lt;/li&gt;&#xA;&lt;li&gt;Support Vector Machines (SVMs)&lt;/li&gt;&#xA;&lt;li&gt;Clustering&lt;/li&gt;&#xA;&lt;li&gt;Dimensionality Reduction&lt;/li&gt;&#xA;&lt;li&gt;Anomaly Detection&lt;/li&gt;&#xA;&lt;li&gt;Recommender Systems&lt;/li&gt;&#xA;&lt;li&gt;Large-Scale Machine Learning&lt;/li&gt;&#xA;&lt;li&gt;Example of an application of machine learning&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Each lecture had a homework assignment to go with it, where we&#xA;implemented the algorithms in Octave. This was crucial to properly&#xA;understand them, and to remember at least the gist of everything. It&amp;rsquo;s&#xA;too easy to zone out when watching a lecture and to convince yourself&#xA;afterward that you got it. In addition, I had never used Octave (or&#xA;Matlab) before and I now realize how useful it is for quick&#xA;development of mathematical code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below are some of my class notes. They are neither complete nor&#xA;formatted well or checked for correctness, so youâ€™d be better off&#xA;taking the class yourself! The last two lectures are missing because I&#xA;had too much other things going on.&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;Really nice intro lectures with lots of motivation and an accessible&#xA;explanation of supervised vs. unsupervised learning.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Notation&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;m := number of training examples&#xA;x&#39;s := input variables&#xA;y&#39;s := output variables&#xA;h := hypothesis := learning algorithm. h(x) = y.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3&gt;Linear regression with one variable&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;h(x) = t0 + t1*x&lt;/code&gt; &amp;mdash; univariate linear regression, straight line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fitting h is a minimization problem over the sum of all x&amp;rsquo;s. For each&#xA;x, minimize the square of the difference between y and h(x). We divide&#xA;that sum by 2m to average. This cost function is called square error&#xA;and is written J(t0, t1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Gradient descent&lt;/em&gt; solves linear regression. Repeat until convergence:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;t_j := t_j - alpha( (delta/delta*t_j) * J(t0, t1) )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Alpha is a learning rate. t0 and t1 must be updated simultaneously,&#xA;assigning to temp variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Batch gradient descent&lt;/em&gt; means we look at all training examples for&#xA;each step.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Linear regression with more than one variable&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Generalized gradient descent: &lt;code&gt;h(x) = theta&#39; * x = t0x0 + t1x1 + ...&lt;/code&gt;.&#xA;We set x0 to 1 always. The parameters t0, t1, &amp;hellip; form a vector theta.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The cost function is then &lt;code&gt;J(theta) = 1/2m * sum_m( (h(x_i)-y_i)^2 )&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The update step is &lt;code&gt;t_j = t_j - alpha * der(t_j) * J(theta)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Scaling gradient descent to make it converge much faster: scale the&#xA;x_i by doing &lt;code&gt;(x_i - avg(x))/range(x)&lt;/code&gt; where range is max - min.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If gradient descent runs correctly, J(theta), the cost function,&#xA;should decrease after every step. If it&amp;rsquo;s increasing or oscillating&#xA;it&amp;rsquo;s usually a too large alpha. Plotting the cost function over the&#xA;number of iterations helps debugging.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ng recommends starting with 0.001 and increasing threefold to 0.003,&#xA;0.01 and so on.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Solving linear regression with the normal equation&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Solving theta analytically. Construct a matrix X (&lt;em&gt;design matrix&lt;/em&gt;)&#xA;that is the table of feature values in the training data, with the&#xA;columns being x0 (=1), x1, etc. X is an m x (n+1) matrix (n for n&#xA;features, +1 for x0). Correspondingly, take y to be the vector of&#xA;targets in the training data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Theta is then &lt;code&gt;(X&#39; X)^-1 X&#39; y&lt;/code&gt;. In Octave: &lt;code&gt;pinv(X&#39;*X) * X&#39; * y&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Classification with logistic regression&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Result variable y of {0, 1}, &lt;em&gt;negative class&lt;/em&gt; and &lt;em&gt;positive class&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Linear regression could be used by setting a threshold for the&#xA;classifier h_theta. But outliers will greatly influence the&#xA;classifier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In logistic regression (which is actually a classification algorithm),&#xA;the prediction h_theta is always between 0 and 1. We call it&#xA;hypothesis and interpret it as the probability that y=1 on input x:&#xA;&lt;code&gt;h_theta(x) = P(y=1 | x; theta)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Where in linear regression h_theta(x) was &lt;code&gt;theta&#39;*x&lt;/code&gt;, it is now&#xA;&lt;code&gt;g(theta&#39;*x)&lt;/code&gt;. g is called the &lt;em&gt;sigmoid function&lt;/em&gt; or &lt;em&gt;logistic&#xA;function&lt;/em&gt; and is defined as &lt;code&gt;g(z) = 1 / 1+e^-z&lt;/code&gt;. It asymptotes at 0 on&#xA;the left and 1 at the right and is 0.5 at zero.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We predict the hypothesis to be true when g(z) &amp;gt;= 0.5, which happens&#xA;when z &amp;gt;= 0, where z is theta&amp;rsquo;*x.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;em&gt;decision boundary&lt;/em&gt; is the line drawn by &lt;code&gt;theta&#39;*x &amp;gt;= 0&lt;/code&gt;. It is a&#xA;property of the hypothesis, not of the data set, so we need to fit the&#xA;parameters to the training set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For non-linear decision boundaries, we can introduce polynomial&#xA;features such as x2^2 (and possibly delete the linear ones by setting&#xA;their parameter in theta to 0). For example, &lt;code&gt;x1^2 + x2^2 = 1&lt;/code&gt; is a&#xA;circle of radius 1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The cost function we used for linear regression, half of the squared&#xA;error, is not convex when used with logistic regression. That means&#xA;gradient descent will get stuck in local minima. Instead, we use the&#xA;convex function &lt;code&gt;Cost(h_theta(x), y) = -log(h_theta(x)) if y=1,&#xA;-log(1-h_theta(x)) if y=0&lt;/code&gt;. This is equivalent to &lt;code&gt;-y*log(h(x)) -&#xA;(1-y)*log(1-h(x))&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The overall cost J(theta) is then &lt;code&gt;1/m sum_i_m Cost(h_theta(x_i),&#xA;y_i)&lt;/code&gt;. It can be derived statistically through maximum likelihood&#xA;estimation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For gradient descent, we take the derivative of J as usual, ending up&#xA;with &lt;code&gt;theta_j = theta_j - alpha * sum_i_m((h(x_i) - y_i) * xj_i)&lt;/code&gt;.&#xA;This is identical to linear regression!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Feature scaling applies to logistic regression just as it does to&#xA;linear regression.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;More sophisticated algorithms than gradient descent&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;conjugate gradient&lt;/li&gt;&#xA;&lt;li&gt;BFGS&lt;/li&gt;&#xA;&lt;li&gt;L-BFGS&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;They figure out the learning rate on their own and converge faster,&#xA;but are more complex.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In Octave xthere&amp;rsquo;s &lt;code&gt;fminunc&lt;/code&gt;&amp;mdash;function minimization unconstrained.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Regularization&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Underfitting = high bias. Polynomial of too low degree in logistic&#xA;regression.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Overfitting = high variance. Polynomial of too high degree in logistic&#xA;regression. Happens with too many features.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When we have a lot of features, plotting the classifier is impossible&#xA;and it becomes hard to detect overfitting. Later we&amp;rsquo;ll see &lt;em&gt;model&#xA;selection&lt;/em&gt; to decide which features to discard.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Discarding features loses information. Instead, we can employ&#xA;&lt;em&gt;regularization&lt;/em&gt;. The idea is to penalize the features (thetas):&#xA;multiply them with a big factor in the cost function forces the&#xA;minimization to make them really small. J(theta) becomes &lt;code&gt;1/2m&#xA;sum_i_m(h(xi)-yi)^2 + lambda sum_j_n(theta_j^2)&lt;/code&gt;. Lambda is the&#xA;regularization factor. The derivative of the added lambda term used in&#xA;the update step is &lt;code&gt;lambda/m * theta_j&lt;/code&gt;. It doesn&amp;rsquo;t depend on the i in&#xA;the sum, so it can be pulled out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using the normal equation, lambda is simply applied to the feature&#xA;matrix.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Neural networks&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Logistic regression blows up with lots of features. There are about&#xA;O(nÂ²) terms if we include only the second-order terms x1x2, x1x3 etc.,&#xA;and O(nÂ³) terms if we include the cubic terms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Some terminology:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;neuron&lt;/em&gt; or &lt;em&gt;bias unit&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;input wires x1, &amp;hellip;, xn, plus x0 = 1.&lt;/li&gt;&#xA;&lt;li&gt;output is h_theta(x) = 1 / (1+e^(-theta&amp;rsquo;&lt;em&gt;x)), the sigmoid function,&#xA;here *activation function&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;theta are the parameters or &lt;em&gt;weights&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;input layer, hidden layers (neither x nor y), output layers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Theta is a matrix since the neural network has a matrix structure (one&#xA;column per layer). If layer j has sj units, and layer k has sk&#xA;units, then theta_j is of dimension sk x sj+1. For example, with&#xA;j=k=2, &lt;code&gt;activation(i, k) = g(theta(i,0)*x0 + theta(i,1)*x1 +&#xA;theta(i,2)*x2)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Neural networks are basically several layers of logistic regression.&#xA;It learns the features from the basic input features x1, x2, &amp;hellip;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The layers, their size etc. are called the architecture of a neural&#xA;network.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To learn theta, we first need a cost function. The cost function is a&#xA;generalization of the one for logistic regression. For K output units,&#xA;we compute h_theta(x) for k = 1,&amp;hellip;,K. We&amp;rsquo;re taking the kth output&#xA;unit and compare it to y_k.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Back propagation: let delta_j_l be the error of node j in layer l. For&#xA;the output layer L, &lt;code&gt;delta_L = a_L - y&lt;/code&gt; (vectorized for all nodes in&#xA;the layer). For the previous layers l, &lt;code&gt;delta_l = (Theta_l&#39; *&#xA;delta_l+1) .* g&#39;(z_l)&lt;/code&gt;. The derivative of the sigmoid at z_l is &lt;code&gt;a_l&#xA;.* (1-a_l)&lt;/code&gt;. There is no delta_1 since this is the input. We start at&#xA;L and go back to 2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The back propagation algorithm is then: for each training example (xi,&#xA;yi) do&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Set a1 = xi&lt;/li&gt;&#xA;&lt;li&gt;Perform forward propagation to compute a_l for l = 2,&amp;hellip;,L.&lt;/li&gt;&#xA;&lt;li&gt;Compute the error delta_L = a_L - y_i.&lt;/li&gt;&#xA;&lt;li&gt;Compute delta_(L-1), &amp;hellip;, delta_2. &lt;code&gt;DELTA_ij_l = DELTA_ij_l +&#xA; a_j_l*delta_i_(l+1)&lt;/code&gt;. The latter expression is vectorized as&#xA; &lt;code&gt;DELTA_l = DELTA_l + delta_(l+1)*a_l&#39;&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Then, &lt;code&gt;DELTA/m&lt;/code&gt; is the partial derivative of the cost function.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Machine Learning Diagnostics&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Your ML approach isn&amp;rsquo;t working so well. What&amp;rsquo;s wrong?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Split data into training and test set, approx. 70 to 30. After&#xA;learning, calculate the average square error on the test set (linear&#xA;regression).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To choose between different models, such as different order&#xA;polynomials for regression, we can learn using each candidate model on&#xA;the training set and choose the one performing best on the test set.&#xA;However, our error will now be optimistic because we picked the best&#xA;model for the test set. To avoid this, we use a third data set for&#xA;cross-validation. We test the model hypotheses on the validation set&#xA;and then estimate the generalization error using the test set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If performance is not good enough, we need to distinguish between&#xA;underfitting (high bias) and overfitting (high variance). In case of a&#xA;bias problem, both the training and the cross-validation error are&#xA;high. In case of high variance, the training error is low (because the&#xA;training set fits very well), but the cv error is high.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Large learning parameters (lambda) will lead to high bias (underfit),&#xA;small ones to high variance.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;How to approach an ML problem&lt;/h2&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Partition the data into training, test, cv.&lt;/li&gt;&#xA;&lt;li&gt;Start with a simple algorithm and validate it.&lt;/li&gt;&#xA;&lt;li&gt;Plot learning curves (comparing average squared error of test and&#xA;cv set) to see whether there is high variance or high bias and&#xA;therefore whether more examples, more features etc. are going to&#xA;help.&lt;/li&gt;&#xA;&lt;li&gt;Error analysis: manually examine the erroneous examples in the cv&#xA;set to spot any systematic errors.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</content>
   <category term="AI&amp;ML"></category>
   <category term="Courses"></category>
  </entry>
  <entry>
   <title>Intro to Artificial Intelligence Online Course</title>
   <link href="http://www.thomaskappler.net/2011-12-22_intro_to_ai_course.html" rel="alternate"></link>
   <updated>2011-12-22T00:00:00Z</updated>
   <id>tag:www.thomaskappler.net,2011-12-22:/2011-12-22_intro_to_ai_course.html</id>
   <summary type="html">The online class was a bold experiment, bringing Stanford teaching by Sebastian Thun and Peter Norvig to over 100,000 students. I loved the experience and got a lot out of it. Here are some thoughts and selected notes.</summary>
   <content type="html">&lt;p&gt;I had always been interested in AI but ended up choosing different&#xA;majors in college. Naturally, this&#xA;&lt;a href=&#34;https://www.ai-class.com/&#34;&gt;online AI class&lt;/a&gt; couldn&amp;rsquo;t make up for that&#xA;by itself given the time constraints, but I feel it did a great job on&#xA;bringing the students up to speed in the important basics of the&#xA;field. The range of topics was wide enough to serve as a solid&#xA;foundation for further reading:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;How to think of environments&lt;/li&gt;&#xA;&lt;li&gt;Searching through state spaces, A* search&lt;/li&gt;&#xA;&lt;li&gt;Probability and Bayes Rule&lt;/li&gt;&#xA;&lt;li&gt;Bayes Networks, D-Separation and probabilistic inference&lt;/li&gt;&#xA;&lt;li&gt;Sampling methods&lt;/li&gt;&#xA;&lt;li&gt;Machine Learning basics and k-nearest neighbors&lt;/li&gt;&#xA;&lt;li&gt;Planning under uncertainty, Markov Decision Processes&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The online format worked great. I enjoyed the little quizzes during&#xA;the course of each lecture because they made me pay more attention and&#xA;helped retention.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Due to work and other constraints I couldn&amp;rsquo;t give the class as much&#xA;time as I wanted, but I still finished with a score of 88%. Nothing&#xA;special given that I have a CS degree, but not too bad either. In any&#xA;case I&amp;rsquo;m proud that I took the time to sit down every week and do&#xA;this, and of having been part of this huge experiment that will&#xA;probably shape the future of learning.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below are some of my class notes. They are not complete and you&amp;rsquo;d be&#xA;better off taking the class yourself&amp;mdash;it&amp;rsquo;s&#xA;&lt;a href=&#34;http://www.udacity.com/overview/Course/cs271&#34;&gt;available on udacity&lt;/a&gt;&#xA;now.&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;h2&gt;1 Welcome to AI&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;AI in a nutshell: an agent observes its environments through sensors,&#xA;and can influence it through actuators. The &lt;em&gt;control plan&lt;/em&gt; mapping&#xA;sensor input to activator output is the focus of AI. The repeated&#xA;cycle of input and output is the &lt;em&gt;Perception Action Cycle&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Classifying environments:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;An environment is &lt;em&gt;fully observable&lt;/em&gt; if an agent can, at all&#xA;times, make all observations to make the optimal choice, i.e., see&#xA;the entire state of the environment (chess). If this is not&#xA;possible and the agent has to memorize previous observations&#xA;(poker), it is &lt;em&gt;partially observable&lt;/em&gt;.&lt;/li&gt;&#xA;&lt;li&gt;It can be deterministic (chess) or stochastic (dice games).&lt;/li&gt;&#xA;&lt;li&gt;The agent&amp;rsquo;s actuators can be discrete or continuous (e.g. throwing&#xA;darts: infinite possibilities of angles and acceleration).&lt;/li&gt;&#xA;&lt;li&gt;It can be benign or adversarial.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2&gt;2 Problem Solving&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Definition of a problem:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;initial state&lt;/li&gt;&#xA;&lt;li&gt;actions(s) = {a1, a2, &amp;hellip;}&lt;/li&gt;&#xA;&lt;li&gt;result(s, a) = s&amp;rsquo;&lt;/li&gt;&#xA;&lt;li&gt;goaltest(s) = T|F&lt;/li&gt;&#xA;&lt;li&gt;pathcost(s-a-&amp;gt;s-a-&amp;gt;s-&amp;hellip;) -&amp;gt; n&lt;/li&gt;&#xA;&lt;li&gt;stepcost(s, a, s&amp;rsquo;) -&amp;gt; n&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;State space&lt;/em&gt; = explored + frontier + unexplored&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Tree search&lt;/em&gt;: until the goal is reached, take a choice from the&#xA;frontier and apply all actions to it. This is a whole family of&#xA;algorithms, since the removal of the choice is critical.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In breadth-first tree search, the three sections of the state space&#xA;correspond to three vertical regions of the tree.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Tree search backtracks, since for each tree node, the one we came from&#xA;is just another frontier node.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can turn the tree search into a graph search by remembering which&#xA;nodes we&amp;rsquo;ve seen already.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In &lt;em&gt;uniform cost search&lt;/em&gt; we pick the cheapest choice first.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For finding the goal in the shortest route as in hops, breadth-first&#xA;is optimal while depth-first is not. For finding the goal via the&#xA;cheapest route, uniform cost is optimal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Uniform cost search is related to topological maps: we fan out from a&#xA;starting point, in circles of increasing distance from the starting&#xA;point.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;A* search&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;f = g + h&lt;/code&gt; where g(p) is the path cost and h(p) = h(final state of p)&#xA;is the estimated distance to the goal.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A* search does not end when the goal is reached&amp;mdash;there might be&#xA;other, shorter paths. The goal test happens after selecting the next&#xA;path to be taken off the frontier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A* finds the lowest cost path if, for all s, h(s) &amp;lt;= true cost. In&#xA;other words, h is &lt;em&gt;optimistic&lt;/em&gt; or &lt;em&gt;admissible&lt;/em&gt;, it never overestimates.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Problem solving works when:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the domain is fully observable,&lt;/li&gt;&#xA;&lt;li&gt;the domain is known (available actions),&lt;/li&gt;&#xA;&lt;li&gt;the domain is discrete (finite number of actions),&lt;/li&gt;&#xA;&lt;li&gt;the domain is deterministic, and&lt;/li&gt;&#xA;&lt;li&gt;the domain is static.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;For the implementation of paths and state spaces the core data&#xA;structure is the &lt;em&gt;node&lt;/em&gt;, with four fields: state, action (it took to&#xA;get there), cost (total), parent (preceding state). A linked list of&#xA;nodes represents a path. The two main data structures dealing with&#xA;nodes are the frontier and the explored list. The frontier is a&#xA;priority queue with an additional set for membership test. The&#xA;explored list can be a simple set.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;3 Probability in AI&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Bayes Networks&lt;/em&gt;: nodes are random variables. Edges signify&#xA;probabilistic influence. It&amp;rsquo;s a compact representation of a&#xA;probability distribution over a very large joint distribution of all&#xA;involved variables. The state space has the size 2^n even if the&#xA;variables were all binary.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If X and Y are independent, &lt;code&gt;P(X,Y) = P(X)*P(Y)&lt;/code&gt;. The P(X) and P(Y)&#xA;are called &lt;em&gt;marginals&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;First, some simple probability review:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Total probability: &lt;code&gt;P(Y) = P(Y|X=i)*P(X=i)&lt;/code&gt; summed over all i.&lt;/li&gt;&#xA;&lt;li&gt;Negation: &lt;code&gt;P(-X|Y) = 1 - P(X|Y)&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Joint probabilities: &lt;code&gt;P(X,Y) = P(Y=y|X=x) * P(X=x)&lt;/code&gt; and vice&#xA;versa.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3&gt;Bayes Rule&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;P(A|B) = P(B|A)\*P(A) / P(B)&lt;/code&gt;. In this formula, P(B|A) is the&#xA;&lt;em&gt;likelihood&lt;/em&gt;, P(A) the &lt;em&gt;prior&lt;/em&gt;, P(B) the &lt;em&gt;marginal likelihood&lt;/em&gt;, and&#xA;P(A|B) the &lt;em&gt;posterior&lt;/em&gt;. The marginal likelihood can be replaced by the&#xA;total probability.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Drawing Bayes Rule with two nodes A and B is a Bayesian network with&#xA;two nodes A-&amp;gt;B!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How many parameters do we need to specify a Bayes Network of two&#xA;variables? Three: P(A) (from which we derive P(-A)), P(B|A), and&#xA;P(B|-A).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Observe that using Bayes Rule, the normalizer is the same for both&#xA;P(A|B) and P(-A|B). The two cases sum to 1. So we can compute Bayes&#xA;Rule in a different way, effectively ignoring the normalizer:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;P&#39;(A|B)  = P(B|A)  P(A)&#xA;P&#39;(-A|B) = P(B|-A) P(-A)&#xA;&#xA;P(A|B)  = &amp;lt;eta&amp;gt;P&#39;(A|B)&#xA;P(-A|B) = &amp;lt;eta&amp;gt;P&#39;(-A|B)&#xA;&#xA;&amp;lt;eta&amp;gt; = 1 / (P&#39;(A|B) + P&#39;(-A|B))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We defer the calculation of the normalizer.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Exploiting &lt;em&gt;conditional independence&lt;/em&gt;: if T1 and T2 with values x and&#xA;y depend on A, what is P(t1|t2), given P(A), P(x|A) and P(y|A)? Using&#xA;total probability, it is &lt;code&gt;P(t2|t1,A) * P(A|t1) + P(t2|t1,-A) *&#xA;P(-A|t1)&lt;/code&gt;. But if we know A and T1 and T2 are independent, knowledge&#xA;of the first test gives me no more information about the second test.&#xA;So we can simplify this to &lt;code&gt;P(t2|A)P(A|t1) + P(t2|-A)P(-A|t1)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Absolute independence does not imply conditional independence, and&#xA;vice versa.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Confounding causes&lt;/em&gt;: two (unobservable) nodes influence one output&#xA;variable. There&amp;rsquo;s no connection between the two influencing&#xA;variables&amp;mdash;if P(A) = x, then P(A|B=b) = x.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Explaining away&lt;/em&gt;: independence does not imply conditional&#xA; independence. If we have&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;S   R&#xA; \ /&#xA;  H&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;then S and R are independent. But if we observe H, they are not&#xA;independent anymore&amp;mdash;its value gives us a clue which one is more&#xA;likely.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Bayes Networks&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The joint probability of a BN is the product of the probabilities of&#xA;its nodes, conditioned on their incoming arcs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How many parameters define a BN? If the above product is &lt;code&gt;P(A) P(B)&#xA;P(C|A,B) P(D|C) P(E|C)&lt;/code&gt;, it&amp;rsquo;s ten. P(A) and P(B) are one each. P(D|C)&#xA;and P(E|C) are two each, for each possible value of C. P(C|A,B) is 4:&#xA;all combinations of A and B.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;D-Separation&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;This is about &lt;em&gt;conditional independence&lt;/em&gt; in BNs, also called&#xA;&lt;em&gt;reachability&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To determine independence of variables in a graph (like a BN), think&#xA;about &amp;ldquo;would knowing X help me to know more about Y?&amp;rdquo; More formally,&#xA;variables are dependent if they are connected by a path of &lt;em&gt;unknown&lt;/em&gt;&#xA;variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;A -&amp;gt; UNKNOWN -&amp;gt; B&lt;/code&gt; is an &lt;em&gt;active triplet&lt;/em&gt;, &lt;code&gt;A -&amp;gt; KNOWN -&amp;gt; B&lt;/code&gt; is&#xA;&lt;em&gt;inactive&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This also goes for upstream relations&amp;mdash;this is the &amp;ldquo;explaining away&amp;rdquo;&#xA;case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;See &lt;a href=&#34;http://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html&#34;&gt;Bayes Ball&lt;/a&gt; for&#xA;an easy test of conditional independence in BNs.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;4 Probabilistic Inference&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Given a BN, we can ask &amp;ldquo;for these inputs, what are the outputs?&amp;rdquo; We&#xA;call the input &lt;em&gt;evidence&lt;/em&gt;, the output &lt;em&gt;query&lt;/em&gt;, and the interior nodes&#xA;&lt;em&gt;hidden variables&lt;/em&gt;. The answer is a complete probability distribution&#xA;over the query variables, called the &lt;em&gt;posterior distribution&lt;/em&gt;: P(Q1,&#xA;Q2, &amp;hellip;|E1=e1, &amp;hellip;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Evidence nodes whose value we don&amp;rsquo;t know, and query nodes we don&amp;rsquo;t&#xA;care about are hidden!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The obvious question is which evidence has the most likely result,&#xA;i.e., the argmax over the above P(&amp;hellip;).&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Enumeration&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Conditional probability&lt;/em&gt;: &lt;code&gt;P(Q|E) = P(Q,E) / P(E)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Enumerating a BN, here for the burglary/earthquake -&amp;gt; alarm -&amp;gt;&#xA;John/Mary calls network. We sum over the probability product for all&#xA;combinations of the hidden variables (&amp;ldquo;Se&amp;rdquo; = sum over e).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Se(Sa( P(+b) P(e) P(A|+b,e)P(+j|a)P(+m,a) ))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Moving invariants out of inner loops:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;P(+b) S( P(e) S(P(A|+b,e)P(+j|a)P(+m,a)) )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That&amp;rsquo;s still not good enough. The next step is to &lt;em&gt;maximize&#xA;independence&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A &amp;ldquo;linear&amp;rdquo; BN where each node points to one other node will take O(n),&#xA;while one where each node is connected to all other nodes will take&#xA;(2^n).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The next optimization is &lt;em&gt;variable elimination&lt;/em&gt; (4.8). First, joining&#xA;factors. Let&amp;rsquo;s say we have P&amp;reg; and P(T|R). We combine them into&#xA;P(R,T) by multiplying their tables together, through all combinations&#xA;of r and t. Then, elimination (also called marginalization): turn&#xA;P(R,T) into P(T) by summing up the P(R,T) table over r.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Approximate Inference Sampling&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Flip coin repeatedly, count outcomes to estimate the joint probability&#xA;distribution. In a BN&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If the count of each sampled variable is chosen according to the probability&#xA;tables, the sampling approaches the true probability distribution with&#xA;infinite samples, then the sampling method is &lt;em&gt;consistent&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Rejection sampling&lt;/em&gt;: to estimate the distribution for a conditional&#xA;variable, we go through the samples and reject all those that don&amp;rsquo;t&#xA;match the condition (evidence). The problem is that if the evidence is&#xA;unlikely we need to reject most samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Likelihood weighting&lt;/em&gt; fixes this problem. We fix the evidence&#xA;variables and sample the rest, so we can keep all samples. However,&#xA;the distribution is inconsistent. To make it consistent, we attach a&#xA;probabilistic weight to each sample. For instance, if rain has a&#xA;probability of 0.4 and we&amp;rsquo;re sampling P(grass wet|rain), we are forced&#xA;to choose the 0.4 probability entry for rain in its table, so we&#xA;weight the sample with 0.4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Gibb&amp;rsquo;s Sampling&lt;/em&gt; takes all evidence into account. It&amp;rsquo;s based on&#xA;Markov Chain Monte Carlo (MCMC). The idea is to sample one variable at&#xA;a time, conditioned on all the others. We initialize all variables for&#xA;the first sample. In each following iteration, we choose one variable&#xA;at random and resample it. So we randomly walk the space of variable&#xA;assignments. Even though each iteration depends on the previous one,&#xA;the method is consistent.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Approximate Inference Sampling&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;We can Flip a coin repeatedly and count the outcomes to estimate the&#xA;joint probability distribution. If the count of each sampled variable&#xA;is chosen according to the probability tables, the sampling approaches&#xA;the true probability distribution with infinite samples, then the&#xA;sampling method is &lt;em&gt;consistent&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Rejection sampling&lt;/em&gt;: to estimate the distribution for a conditional&#xA;variable, we go through the samples and reject all those that don&amp;rsquo;t&#xA;match the condition (evidence). The problem is that if the evidence is&#xA;unlikely we need to reject most samples.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Likelihood weighting&lt;/em&gt; fixes this problem. We fix the evidence&#xA;variables and sample the rest, so we can keep all samples. However,&#xA;the distribution is inconsistent. To make it consistent, we attach a&#xA;probabilistic weight to each sample. For instance, if rain has a&#xA;probability of 0.4 and we&amp;rsquo;re sampling P(grass wet|rain), we are forced&#xA;to choose the 0.4 probability entry for rain in its table, so we&#xA;weight the sample with 0.4.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Gibb&amp;rsquo;s Sampling&lt;/em&gt; takes all evidence into account. It&amp;rsquo;s based on&#xA;Markov Chain Monte Carlo (MCMC). The idea is to sample one variable at&#xA;a time, conditioned on all the others. We initialize all variables for&#xA;the first sample. In each following iteration, we choose one variable&#xA;at random and resample it. So we randomly walk the space of variable&#xA;assignments. Even though each iteration depends on the previous one,&#xA;the method is consistent.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;5 Machine Learning&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Machine learning&lt;/em&gt; is learning models from data. (Later note: it&amp;rsquo;s&#xA;interesting to compare this definition with Andrew Ng&amp;rsquo;s from the&#xA;Stanford Machine Learning class on coursera: giving computers the&#xA;ability to learn without being explicitly programmed.)&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Supervised Learning&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Given a &lt;em&gt;feature vector&lt;/em&gt; &lt;code&gt;x1, ..., xn -&amp;gt; y&lt;/code&gt;, y is the &lt;em&gt;target label&lt;/em&gt; or&#xA;&lt;em&gt;predictor&lt;/em&gt;. A set of feature vectors with their target labels is&#xA;called the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Occam&amp;rsquo;s Razor&lt;/em&gt;: prefer the simpler hypothesis. There is a conflict&#xA;between generalization error and training data error. Minimizing the&#xA;latter leads to overfitting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Maximum Likelihood&lt;/em&gt;: given a number of discrete data points y_i, find&#xA;the P(x) that maximizes the likelihood of x in the data. With two&#xA;discrete outcomes H and S: &lt;code&gt;p(S) = q&lt;/code&gt;, &lt;code&gt;p(y_i) = q&lt;/code&gt; if y_i=S, 1-q&#xA;otherwise. Then &lt;code&gt;p(data) = multover(p(y_i) = q^(count(y_i=H) *&#xA;(1-q)^(count(Y_i=S))&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can do the counts, now we have to determine q. Maximizing p(data)&#xA;is equivalent to maximizing log(p(data)).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can construct a BN modeling the problem. If we have a dictionary&#xA;with 12 words, the BN will have 23 parameters: 1 for P(spam), and 12&#xA;each for the spam and ham distributions. The parameters are estimated&#xA;by supervised learning using an ML estimator with training data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To determine whether a message is spam, given the prior P(S), we&#xA;multiply the prior with the probabilities of each word of occuring in&#xA;a spam message, and divide by this expression plus the equivalent one&#xA;for ham (total probability). Normal Bayes Theorem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When determing the p(w) that a word is spam by counting its occurences&#xA;in the test data spam, we risk overfitting. When a word does not occur&#xA;in the test data, the P(S) of a message containing that word will&#xA;always be 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Laplace smoothing&lt;/em&gt; addresses this. Instead of p(x) = count(x)/N, we&#xA;add a smoothing factor: &lt;code&gt;p(x) = count(x)+k / count(x) + N&lt;/code&gt;, where N is&#xA;the number of total different values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cross-validation: find the Laplacian k by cross-validating repeatedly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Supervised learning can be classification (what we&amp;rsquo;ve done until now),&#xA;and regression: make continuous predictions, such as for tomorrow&amp;rsquo;s&#xA;temperature.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Suggestively: what&amp;rsquo;s the best curve through the data points? The one&#xA;that hits each point would be good, but it&amp;rsquo;s overfit of course.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Linear regression formally: Given x11, &amp;hellip;, x1n -&amp;gt; y1 to xm1, &amp;hellip;, xmn&#xA;-&amp;gt; ym, we want f(x) = y. In two dimensions: f(x) = w1*x + w0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We minimize the loss function modeling the residual error.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LOSS = sum_j(yj - w1*xj - w0)^2.&lt;/li&gt;&#xA;&lt;li&gt;w1* = argmin_w(L)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Because we minimize quadratic error, outliers penalize&#xA;disproportionately.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Logistic regression: if f(x) is a linear regression, z = 1 / 1 +&#xA;e^(-f(x)).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Gradient descent: iterative method. The i+1 guess is the ith guess&#xA;minus its gradient times a small alpha (0.01). Gradient descent gets&#xA;trapped in local minima.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Which linear separator (if there is one) is preferrable? The one that&#xA;maximizes the margin, i.e., the distance to the closer data&#xA;points. This ensures that future real data will probably be on the&#xA;right side. SVMs and Boosting are popular to find this separator.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SVMs work with kernels to enable linear separation. For instance, if&#xA;points are circular around the origin, and class + is closer than&#xA;class -, there is no linear separator. But if we map the points to&#xA;their distance from origin, there is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far, all methods of supervised learning were parametric. The number&#xA;of parameters was independent of the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In non-parametric methods, the number of params can grow over time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Conformate&lt;/em&gt; plans are plans that are guaranteed to reach a goal&#xA;without sensing the world.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;9 Planning under uncertainty&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;We&amp;rsquo;ve covered planning, uncertainty, and learning before, but not in&#xA;combination. Where planning and uncertainty intersect, we will look at&#xA;&lt;em&gt;(Partially observable) Markov Decision Processes&lt;/em&gt; (MDPs). In&#xA;combination with learning, we&amp;rsquo;ll look at &lt;em&gt;Reinforcement Learning&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;MDPs&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;We have states, actions, and a state transition matrix where&#xA;&lt;code&gt;T(s,a,s&#39;) = P(s&#39;|a,s)&lt;/code&gt;. A reward function R(s) defines the goal to&#xA;achieve. For now, the rewards are simply attached to states. The&#xA;problem is then to attach an action to each state so that R(s) is&#xA;maximized.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A &lt;em&gt;policy&lt;/em&gt; assigns actions to any state (except absorbing states).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a stochastic environment, what&amp;rsquo;s the problem with the tree approach&#xA;of conventional planning? Large branching factor, deep trees, many&#xA;states visited more than once.&lt;/p&gt;&#xA;</content>
   <category term="AI&amp;ML"></category>
   <category term="Courses"></category>
  </entry>
 </feed>